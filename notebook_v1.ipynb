{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c62d882",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "38ee4e58",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-44-86f7860897ce>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCounter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregularizers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0ml2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membeddings\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mEmbedding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "# core system imports\n",
    "import os\n",
    "\n",
    "import string # Remove punctuation\n",
    "import pandas as pd # Managing dataframe\n",
    "import numpy as np # Handling mathematical operations on array\n",
    "import pickle # library for saving the state of your dataframe\n",
    "from collections import Counter\n",
    "\n",
    "from keras import optimizers\n",
    "from keras.regularizers import l2\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import (Sequential, load_model)\n",
    "\n",
    "from keras.layers import (\n",
    "    Dense,\n",
    "    Dropout,\n",
    "    LSTM,\n",
    "    Conv1D, \n",
    "    MaxPooling1D, \n",
    "    Flatten,\n",
    "    TimeDistributed\n",
    ")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from DataPrep.Clean_Texts import clean_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a5332360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Location of the pipeline metadata store\n",
    "_pipeline_root = './pipeline/'\n",
    "\n",
    "# Directory of the raw data files\n",
    "_data_root = './data/hausa'\n",
    "\n",
    "_data_filepath = os.path.join(_data_root, \"data.csv\")\n",
    "_stopwords_filepath = os.path.join(_data_root, \"stopwords.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "55cadc84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data.csv', 'stopwords.txt']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List datasets in directory\n",
    "os.listdir(_data_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e7038fdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>author_id</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>- @aishambuhari ta nemi babban sufeton 'yan sa...</td>\n",
       "      <td>7.970000e+17</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"Duk dan Bokon da baida Ilimin Addini Annoba n...</td>\n",
       "      <td>2.290470e+09</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"Duk mutumin da yayi tunanin bawa mutane ilimi...</td>\n",
       "      <td>1.071387e+09</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"Duk wanda ya sabawa dokar kaucewa kamuwa daga...</td>\n",
       "      <td>1.260000e+18</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"Duk wanda ya san ya fito daga yankin da ake A...</td>\n",
       "      <td>1.039268e+09</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text     author_id     Label\n",
       "0  - @aishambuhari ta nemi babban sufeton 'yan sa...  7.970000e+17   Neutral\n",
       "1  \"Duk dan Bokon da baida Ilimin Addini Annoba n...  2.290470e+09   Neutral\n",
       "2  \"Duk mutumin da yayi tunanin bawa mutane ilimi...  1.071387e+09   Neutral\n",
       "3  \"Duk wanda ya sabawa dokar kaucewa kamuwa daga...  1.260000e+18  Positive\n",
       "4  \"Duk wanda ya san ya fito daga yankin da ake A...  1.039268e+09  Positive"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read data from CSV file\n",
    "data = pd.read_csv(_data_filepath)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f50b0ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read stop words\n",
    "stopwords_list = list()\n",
    "\n",
    "with open(_stopwords_filepath) as file:\n",
    "    stopwords_list = [line.strip() for line in file.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2b3ea042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ta da ya sai ba yi na kuma ma ji cikin in ni wata wani ce tana don za sun amma ga ina ne mai suka wannan a ko lokacin su take shi yake yana ka ban ita tafi\n"
     ]
    }
   ],
   "source": [
    "listToStr = ' '.join([str(word) for word in stopwords_list])\n",
    "print(listToStr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6c1cfeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing stopwords\n",
    "def apply_lowercase(text):\n",
    "    text = [item for item in text if item not in stopwords_list]\n",
    "    text = ''.join(text)\n",
    "    return text\n",
    "\n",
    "# removing stopwords\n",
    "def stopwords_removal(text):\n",
    "    text = [item for item in text if item not in stopwords_list]\n",
    "    text = ''.join(text)\n",
    "    return text\n",
    "\n",
    "# remove punctuations\n",
    "def punctuation_removal(text):\n",
    "    all_list = [char for char in text if char not in string.punctuation]\n",
    "    clean_str = ''.join(all_list)\n",
    "    return clean_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9ed68001",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>author_id</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>- @aishambuhari ta nemi babban sufeton 'yan sa...</td>\n",
       "      <td>7.970000e+17</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"Duk dan Bokon da baida Ilimin Addini Annoba n...</td>\n",
       "      <td>2.290470e+09</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"Duk mutumin da yayi tunanin bawa mutane ilimi...</td>\n",
       "      <td>1.071387e+09</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"Duk wanda ya sabawa dokar kaucewa kamuwa daga...</td>\n",
       "      <td>1.260000e+18</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"Duk wanda ya san ya fito daga yankin da ake A...</td>\n",
       "      <td>1.039268e+09</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text     author_id     Label\n",
       "0  - @aishambuhari ta nemi babban sufeton 'yan sa...  7.970000e+17   Neutral\n",
       "1  \"Duk dan Bokon da baida Ilimin Addini Annoba n...  2.290470e+09   Neutral\n",
       "2  \"Duk mutumin da yayi tunanin bawa mutane ilimi...  1.071387e+09   Neutral\n",
       "3  \"Duk wanda ya sabawa dokar kaucewa kamuwa daga...  1.260000e+18  Positive\n",
       "4  \"Duk wanda ya san ya fito daga yankin da ake A...  1.039268e+09  Positive"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f4315c07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>author_id</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>bdulrm ymern ki di kje k cigb d wnke hnnu kwi ...</td>\n",
       "      <td>7.540000e+17</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text     author_id     Label\n",
       "57  bdulrm ymern ki di kje k cigb d wnke hnnu kwi ...  7.540000e+17  Positive"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Change string to lower case\n",
    "data['text'] = data['text'].apply(str.lower)\n",
    "data.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1fd904b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>author_id</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>mbinhmzt ci gb d wnke hnnu 😂😂😂😂</td>\n",
       "      <td>9.020000e+17</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               text     author_id     Label\n",
       "90  mbinhmzt ci gb d wnke hnnu 😂😂😂😂  9.020000e+17  Positive"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove punctuations or special characters\n",
    "data['text'] = data['text'].apply(punctuation_removal)\n",
    "data.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a7a5a078",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>author_id</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>bdulfthdnn eh we sunce meye n sk msk en\\nwe in...</td>\n",
       "      <td>8.990000e+17</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text     author_id    Label\n",
       "58  bdulfthdnn eh we sunce meye n sk msk en\\nwe in...  8.990000e+17  Neutral"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove stopwords\n",
    "data['text'] = data['text'].apply(stopwords_removal)\n",
    "data.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1d4190a6",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'target'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-5c04cd539c12>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     37\u001b[0m '''\n\u001b[0;32m     38\u001b[0m \u001b[1;31m#How many fake and real articles?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'target'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'target'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkind\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"bar\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mgroupby\u001b[1;34m(self, by, axis, level, as_index, sort, group_keys, squeeze, observed, dropna)\u001b[0m\n\u001b[0;32m   6715\u001b[0m         \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_axis_number\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6716\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6717\u001b[1;33m         return DataFrameGroupBy(\n\u001b[0m\u001b[0;32m   6718\u001b[0m             \u001b[0mobj\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6719\u001b[0m             \u001b[0mkeys\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mby\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\groupby\\groupby.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, obj, keys, axis, level, grouper, exclusions, selection, as_index, sort, group_keys, squeeze, observed, mutated, dropna)\u001b[0m\n\u001b[0;32m    558\u001b[0m             \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrouper\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mget_grouper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    559\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 560\u001b[1;33m             grouper, exclusions, obj = get_grouper(\n\u001b[0m\u001b[0;32m    561\u001b[0m                 \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    562\u001b[0m                 \u001b[0mkeys\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\groupby\\grouper.py\u001b[0m in \u001b[0;36mget_grouper\u001b[1;34m(obj, key, axis, level, sort, observed, mutated, validate, dropna)\u001b[0m\n\u001b[0;32m    809\u001b[0m                 \u001b[0min_axis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgpr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgpr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    810\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 811\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgpr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    812\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgpr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mGrouper\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mgpr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkey\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    813\u001b[0m             \u001b[1;31m# Add key to exclusions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'target'"
     ]
    }
   ],
   "source": [
    "#We will shuffle the data to prevent bias:\n",
    "from sklearn.utils import shuffle\n",
    "data = shuffle(data)\n",
    "data = data.reset_index(drop=True)\n",
    "\n",
    "#Convert the text to lowercase:\n",
    "data['text'] = data['text'].apply(lambda x: x.lower())\n",
    "\n",
    "\n",
    "\n",
    "data['text'] = data['text'].apply(punctuation_removal)\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "#Data Exploration\n",
    "#How many articles per subject?\n",
    "#%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "print(data.groupby(['subject'])['text'].count())\n",
    "data.groupby(['subject'])['text'].count().plot(kind=\"bar\")\n",
    "plt.show()\n",
    "'''\n",
    "#How many fake and real articles?\n",
    "print(data.groupby(['target'])['text'].count())\n",
    "data.groupby(['target'])['text'].count().plot(kind=\"bar\")\n",
    "plt.show()\n",
    "'''\n",
    "#Word Cloud for Negative keywords:\n",
    "from wordcloud import WordCloud\n",
    "fake_data = data[data[\"target\"] == \"fake\"]\n",
    "all_words = ' '.join([text for text in fake_data.text])\n",
    "wordcloud = WordCloud(width= 800, height= 500,\n",
    "                          max_font_size = 110,\n",
    "                          collocations = False).generate(all_words)\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "#Word Cloud for real news:\n",
    "real_data = data[data[\"target\"] == \"true\"]\n",
    "all_words = ' '.join([text for text in fake_data.text])\n",
    "wordcloud = WordCloud(width= 800, height= 500, max_font_size = 110,\n",
    " collocations = False).generate(all_words)\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "'''\n",
    "#Most frequent words function:\n",
    "# Most frequent words counter (Code adapted from https://www.kaggle.com/rodolfoluna/fake-news-detector)   \n",
    "import nltk\n",
    "import seaborn as sns\n",
    "from nltk import tokenize\n",
    "token_space = tokenize.WhitespaceTokenizer()\n",
    "def counter(text, column_text, quantity):\n",
    "    all_words = ' '.join([text for text in text[column_text]])\n",
    "    token_phrase = token_space.tokenize(all_words)\n",
    "    frequency = nltk.FreqDist(token_phrase)\n",
    "    df_frequency = pd.DataFrame({\"Word\": list(frequency.keys()),\n",
    "                                   \"Frequency\": list(frequency.values())})\n",
    "    df_frequency = df_frequency.nlargest(columns = \"Frequency\", n = quantity)\n",
    "    plt.figure(figsize=(12,8))\n",
    "    ax = sns.barplot(data = df_frequency, x = \"Word\", y = \"Frequency\", color = 'blue')\n",
    "    ax.set(ylabel = \"Count\")\n",
    "    plt.xticks(rotation='vertical')\n",
    "    plt.show()\n",
    "        \n",
    "#Most frequent words in fake news:   \n",
    "counter(data[data[\"target\"] == \"data1\"], \"text\", 20)\n",
    "\n",
    "#Most frequent words in real news:\n",
    "#counter(data[data[\"target\"] == \"true\"], \"text\", 20)\n",
    "'''\n",
    "\"\"\"\n",
    "Modeling\n",
    "The modeling process will consist of vectorizing the corpus stored in the â€œtextâ€ column, then applying TF-IDF, and finally a classification machine learning algorithm. Pretty standard in text analytics and NLP.\n",
    "For modeling, we have this function to plot the confusion matrix of the models\"\"\"\n",
    "# Function to plot the confusion matrix (code from https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html)\n",
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "#Split the data:\n",
    "X_train,X_test,y_train,y_test = train_test_split(data['text'], data.target, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "#Logistic Regression\n",
    "# Vectorizing and applying TF-IDF\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics import accuracy_score\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "pipe = Pipeline([('vect', CountVectorizer()),\n",
    "                 ('tfidf', TfidfTransformer()),\n",
    "                 ('model', LogisticRegression())])\n",
    "\n",
    "\n",
    "# Fitting the model\n",
    "model = pipe.fit(X_train, y_train)\n",
    "# Accuracy\n",
    "prediction = model.predict(X_test)\n",
    "print(\" \")\n",
    "print(\"Logistic Regression\")\n",
    "print(\"accuracy: {}%\".format(round(accuracy_score(y_test, prediction)*100,2)))\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "cm = metrics.confusion_matrix(y_test, prediction)\n",
    "plot_confusion_matrix(cm, classes=['Fake', 'Real'])\n",
    "\n",
    "#Decision Tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Vectorizing and applying TF-IDF\n",
    "pipe = Pipeline([('vect', CountVectorizer()),\n",
    "                 ('tfidf', TfidfTransformer()),\n",
    "                 ('model', DecisionTreeClassifier(criterion= 'entropy',\n",
    "                                           max_depth = 20, \n",
    "                                           splitter='best', \n",
    "                                           random_state=42))])\n",
    "# Fitting the model\n",
    "model = pipe.fit(X_train, y_train)\n",
    "# Accuracy\n",
    "prediction = model.predict(X_test)\n",
    "print(\"Deciaion Tree\")\n",
    "print(\"accuracy: {}%\".format(round(accuracy_score(y_test, prediction)*100,2)))\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "cm = metrics.confusion_matrix(y_test, prediction)\n",
    "plot_confusion_matrix(cm, classes=['Fake', 'Real'])\n",
    "\n",
    "# Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "pipe = Pipeline([('vect', CountVectorizer()),\n",
    "                 ('tfidf', TfidfTransformer()),\n",
    "                 ('model', RandomForestClassifier(n_estimators=50, criterion=\"entropy\"))])\n",
    "model = pipe.fit(X_train, y_train)\n",
    "prediction = model.predict(X_test)\n",
    "print(\"Random Forest\")\n",
    "print(\" \")\n",
    "print(\"accuracy: {}%\".format(round(accuracy_score(y_test, prediction)*100,2)))\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "\n",
    "cm = metrics.confusion_matrix(y_test, prediction)\n",
    "plot_confusion_matrix(cm, classes=['Fake', 'Real'])\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db461951",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
