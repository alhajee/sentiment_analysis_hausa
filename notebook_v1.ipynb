{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb2ff638",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55704201",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-36fe5bebf850>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCounter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregularizers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0ml2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membeddings\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mEmbedding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "# core system imports\n",
    "import os\n",
    "\n",
    "import pandas as pd # Managing dataframe\n",
    "import numpy as np # Handling mathematical operations on array\n",
    "import pickle # library for saving the state of your dataframe\n",
    "from collections import Counter\n",
    "\n",
    "from keras import optimizers\n",
    "from keras.regularizers import l2\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import (Sequential, load_model)\n",
    "\n",
    "from keras.layers import (\n",
    "    Dense,\n",
    "    Dropout,\n",
    "    LSTM,\n",
    "    Conv1D, \n",
    "    MaxPooling1D, \n",
    "    Flatten,\n",
    "    TimeDistributed\n",
    ")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from DataPrep.Clean_Texts import clean_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd303182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from CSV file\n",
    "data = pd.read_csv('./data/data1.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24763c29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4190a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Then we add a flag to track fake and real:\n",
    "data['target'] = 'Positive'\n",
    "data['target'] = 'Negative'\n",
    "data['target'] = 'Neutral'\n",
    "\n",
    "#Now lets concatenate the data frames:\n",
    "#data = pd.concat([fake, true]).reset_index(drop = True)\n",
    "\n",
    "#We will shuffle the data to prevent bias:\n",
    "from sklearn.utils import shuffle\n",
    "data = shuffle(data)\n",
    "data = data.reset_index(drop=True)\n",
    "\n",
    "#Data cleansing\n",
    "#Removing the date (we wonâ€™t use it for the analysis):\n",
    "#data.drop([\"date\"],axis=1,inplace=True)\n",
    "\n",
    "#Removing the title (we will only use the text):\n",
    "#data.drop([\"title\"],axis=1,inplace=True)\n",
    "\n",
    "#Convert the text to lowercase:\n",
    "data['text'] = data['text'].apply(lambda x: x.lower())\n",
    "\n",
    "#Remove punctuation:\n",
    "import string\n",
    "def punctuation_removal(text):\n",
    "    all_list = [char for char in text if char not in string.punctuation]\n",
    "    clean_str = ''.join(all_list)\n",
    "    return clean_str\n",
    "data['text'] = data['text'].apply(punctuation_removal)\n",
    "from matplotlib import pyplot as plt\n",
    "'''\n",
    "#Remove stopwords:\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "data['text'] = data['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "\n",
    "\n",
    "#Data Exploration\n",
    "#How many articles per subject?\n",
    "#%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "print(data.groupby(['subject'])['text'].count())\n",
    "data.groupby(['subject'])['text'].count().plot(kind=\"bar\")\n",
    "plt.show()\n",
    "'''\n",
    "#How many fake and real articles?\n",
    "print(data.groupby(['target'])['text'].count())\n",
    "data.groupby(['target'])['text'].count().plot(kind=\"bar\")\n",
    "plt.show()\n",
    "'''\n",
    "#Word Cloud for fake news:\n",
    "from wordcloud import WordCloud\n",
    "fake_data = data[data[\"target\"] == \"fake\"]\n",
    "all_words = ' '.join([text for text in fake_data.text])\n",
    "wordcloud = WordCloud(width= 800, height= 500,\n",
    "                          max_font_size = 110,\n",
    "                          collocations = False).generate(all_words)\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "#Word Cloud for real news:\n",
    "real_data = data[data[\"target\"] == \"true\"]\n",
    "all_words = ' '.join([text for text in fake_data.text])\n",
    "wordcloud = WordCloud(width= 800, height= 500, max_font_size = 110,\n",
    " collocations = False).generate(all_words)\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "'''\n",
    "#Most frequent words function:\n",
    "# Most frequent words counter (Code adapted from https://www.kaggle.com/rodolfoluna/fake-news-detector)   \n",
    "import nltk\n",
    "import seaborn as sns\n",
    "from nltk import tokenize\n",
    "token_space = tokenize.WhitespaceTokenizer()\n",
    "def counter(text, column_text, quantity):\n",
    "    all_words = ' '.join([text for text in text[column_text]])\n",
    "    token_phrase = token_space.tokenize(all_words)\n",
    "    frequency = nltk.FreqDist(token_phrase)\n",
    "    df_frequency = pd.DataFrame({\"Word\": list(frequency.keys()),\n",
    "                                   \"Frequency\": list(frequency.values())})\n",
    "    df_frequency = df_frequency.nlargest(columns = \"Frequency\", n = quantity)\n",
    "    plt.figure(figsize=(12,8))\n",
    "    ax = sns.barplot(data = df_frequency, x = \"Word\", y = \"Frequency\", color = 'blue')\n",
    "    ax.set(ylabel = \"Count\")\n",
    "    plt.xticks(rotation='vertical')\n",
    "    plt.show()\n",
    "        \n",
    "#Most frequent words in fake news:   \n",
    "counter(data[data[\"target\"] == \"data1\"], \"text\", 20)\n",
    "\n",
    "#Most frequent words in real news:\n",
    "#counter(data[data[\"target\"] == \"true\"], \"text\", 20)\n",
    "'''\n",
    "\"\"\"\n",
    "Modeling\n",
    "The modeling process will consist of vectorizing the corpus stored in the â€œtextâ€ column, then applying TF-IDF, and finally a classification machine learning algorithm. Pretty standard in text analytics and NLP.\n",
    "For modeling, we have this function to plot the confusion matrix of the models\"\"\"\n",
    "# Function to plot the confusion matrix (code from https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html)\n",
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "#Split the data:\n",
    "X_train,X_test,y_train,y_test = train_test_split(data['text'], data.target, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "#Logistic Regression\n",
    "# Vectorizing and applying TF-IDF\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics import accuracy_score\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "pipe = Pipeline([('vect', CountVectorizer()),\n",
    "                 ('tfidf', TfidfTransformer()),\n",
    "                 ('model', LogisticRegression())])\n",
    "\n",
    "\n",
    "# Fitting the model\n",
    "model = pipe.fit(X_train, y_train)\n",
    "# Accuracy\n",
    "prediction = model.predict(X_test)\n",
    "print(\" \")\n",
    "print(\"Logistic Regression\")\n",
    "print(\"accuracy: {}%\".format(round(accuracy_score(y_test, prediction)*100,2)))\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "cm = metrics.confusion_matrix(y_test, prediction)\n",
    "plot_confusion_matrix(cm, classes=['Fake', 'Real'])\n",
    "\n",
    "#Decision Tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Vectorizing and applying TF-IDF\n",
    "pipe = Pipeline([('vect', CountVectorizer()),\n",
    "                 ('tfidf', TfidfTransformer()),\n",
    "                 ('model', DecisionTreeClassifier(criterion= 'entropy',\n",
    "                                           max_depth = 20, \n",
    "                                           splitter='best', \n",
    "                                           random_state=42))])\n",
    "# Fitting the model\n",
    "model = pipe.fit(X_train, y_train)\n",
    "# Accuracy\n",
    "prediction = model.predict(X_test)\n",
    "print(\"Deciaion Tree\")\n",
    "print(\"accuracy: {}%\".format(round(accuracy_score(y_test, prediction)*100,2)))\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "cm = metrics.confusion_matrix(y_test, prediction)\n",
    "plot_confusion_matrix(cm, classes=['Fake', 'Real'])\n",
    "\n",
    "# Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "pipe = Pipeline([('vect', CountVectorizer()),\n",
    "                 ('tfidf', TfidfTransformer()),\n",
    "                 ('model', RandomForestClassifier(n_estimators=50, criterion=\"entropy\"))])\n",
    "model = pipe.fit(X_train, y_train)\n",
    "prediction = model.predict(X_test)\n",
    "print(\"Random Forest\")\n",
    "print(\" \")\n",
    "print(\"accuracy: {}%\".format(round(accuracy_score(y_test, prediction)*100,2)))\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "\n",
    "cm = metrics.confusion_matrix(y_test, prediction)\n",
    "plot_confusion_matrix(cm, classes=['Fake', 'Real'])\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
