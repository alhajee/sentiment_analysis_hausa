{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3fc097b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd4cad8",
   "metadata": {},
   "source": [
    "## Import relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7680b9a9",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (Temp/ipykernel_30976/269759905.py, line 42)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\PHUCKM~1\\AppData\\Local\\Temp/ipykernel_30976/269759905.py\"\u001b[1;36m, line \u001b[1;32m42\u001b[0m\n\u001b[1;33m    roc_curve,\u001b[0m\n\u001b[1;37m            ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# core system imports\n",
    "import os\n",
    "\n",
    "import numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import string\n",
    "import random\n",
    "import joblib\n",
    "import itertools\n",
    "\n",
    "from timeit import timeit\n",
    "from unidecode import unidecode\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import CSVLogger\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import (\n",
    "    Dense, \n",
    "    Input, \n",
    "    LSTM, \n",
    "    Embedding, \n",
    "    Dropout, \n",
    "    GlobalMaxPool1D\n",
    ")\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import (\n",
    "    validation_curve,\n",
    "    learning_curve\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    roc_curve,\n",
    "    roc_auc_score,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    accuracy_score,\n",
    "    precision_recall_fscore_support\n",
    ")\n",
    "\n",
    "# Matplotlib config\n",
    "%matplotlib inline\n",
    "%alias_magic t timeit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad9ccbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for available GPU\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe767c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Location of the pipeline metadata store\n",
    "_pipeline_root = '../pipeline/'\n",
    "\n",
    "# Directory of the raw data files\n",
    "_data_root = '../input/'\n",
    "\n",
    "# Directory of the pretrained word embeddings\n",
    "_embedding_root = '../input/embeddings'\n",
    "\n",
    "_data_filepath = os.path.join(_data_root, \"data.csv\")\n",
    "\n",
    "_stopwords_filepath = os.path.join(_data_root, \"stopwords.txt\")\n",
    "\n",
    "_embedding_model_filepath = os.path.join(_embedding_root, \"model_sg\")\n",
    "_pretrained_vectors_filepath = os.path.join(_embedding_root, \"model_sg.wv.vectors.npy\")\n",
    "_neg_vectors_filepath = os.path.join(_embedding_root, \"model_sg.trainables.syn1neg.npy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70a3c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List input datasets in directory\n",
    "os.listdir(_data_root)\n",
    "\n",
    "# Word embeddings\n",
    "print(os.listdir(_embedding_root))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e4fecb",
   "metadata": {},
   "source": [
    "## Load training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9bed6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from CSV file\n",
    "data = pd.read_csv(_data_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b20ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da93fa3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d03f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6be05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read stop words\n",
    "stopwords_list = list()\n",
    "\n",
    "with open(_stopwords_filepath) as file:\n",
    "    stopwords_list = [line.strip() for line in file.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6400176a",
   "metadata": {},
   "outputs": [],
   "source": [
    "listToStr = ' '.join([str(word) for word in stopwords_list])\n",
    "print(listToStr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa6539c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read saved data from disk\n",
    "def load_pickle(filename):\n",
    "    data = joblib.load(filename)\n",
    "    return(data)\n",
    "    \n",
    "# Save data to disk for future use\n",
    "def save_pickle(data, filename):\n",
    "    joblib.dump(data, filename)\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef00641c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing stopwords\n",
    "def _apply_lowercase(text):\n",
    "    text = [item for item in text if item not in stopwords_list]\n",
    "    text = ''.join(text)\n",
    "    return text\n",
    "\n",
    "# removing stopwords\n",
    "def _stopwords_removal(text):\n",
    "    text = [item for item in text if item not in stopwords_list]\n",
    "    text = ''.join(text)\n",
    "    return text\n",
    "\n",
    "# remove punctuations\n",
    "def _punctuation_removal(text):\n",
    "    all_list = [char for char in text if char not in string.punctuation]\n",
    "    clean_str = ''.join(all_list)\n",
    "    return clean_str\n",
    "\n",
    "# Shuffle dataset\n",
    "def _shuffle_dataset(dataset):\n",
    "    dataset = shuffle(data)\n",
    "    dataset = data.reset_index(drop=True)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f914d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change string to lower case\n",
    "data['text'] = data['text'].apply(str.lower)\n",
    "\n",
    "# Change label to lower case\n",
    "data['Label'] = data['Label'].apply(str.lower)\n",
    "\n",
    "# remove punctuations or special characters\n",
    "data['text'] = data['text'].apply(_punctuation_removal)\n",
    "\n",
    "# remove stopwords\n",
    "data['text'] = data['text'].apply(_stopwords_removal)\n",
    "\n",
    "# Shuffle the dataset to prevent bias:\n",
    "data = _shuffle_dataset(data)\n",
    "\n",
    "# Print head of the data\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4dc0aa",
   "metadata": {},
   "source": [
    "### Encode output lalbel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252c121f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make copy of dataframe\n",
    "data_encoded = data.copy()\n",
    "\n",
    "# create a label encoder\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(data_encoded['Label'])\n",
    "\n",
    "data_encoded['label_encoded'] = label_encoder.transform(data_encoded['Label'])\n",
    "\n",
    "data_encoded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb16d65",
   "metadata": {},
   "source": [
    "### Split Dataset\n",
    "\n",
    "+ Tranining: 70% of the dataset\n",
    "+ Testing: 30% of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078ec857",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_classes = [\"label_encoded\"]\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(data_encoded[\"text\"], data_encoded[list_classes], test_size=0.3, random_state = 1)\n",
    "\n",
    "Y_train = Y_train.values\n",
    "Y_test = Y_test.values\n",
    "\n",
    "# Show dimension of the comments\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb88c273",
   "metadata": {},
   "source": [
    "### Tokenize Sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ba027e",
   "metadata": {},
   "source": [
    "To be able to train our model with a text data, we'd have to convert it into number form, for this we're going to use the Tokenizer module from Keras.preprocessing library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fce9302",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_sentences_train = X_train.values\n",
    "list_sentences_test = X_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0188ca71",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(lower=True)\n",
    "\n",
    "tokenizer.fit_on_texts(list(list_sentences_train))\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1cfc3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# size of training vocabulary (number of unique words)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32be4f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out a random sequence of text from the tokenized training data\n",
    "print(random.choice(list_tokenized_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f29ca5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(list_tokenized_train), 'train sequences')\n",
    "print(len(list_tokenized_test), 'test sequences')\n",
    "\n",
    "print('Average train sequence length: {}'.format(np.mean(list(map(len, list_tokenized_train)), dtype=int)))\n",
    "print('Average test sequence length: {}'.format(np.mean(list(map(len, list_tokenized_test)), dtype=int)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d97b93",
   "metadata": {},
   "source": [
    "## Pad tokenized Sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb53a50",
   "metadata": {},
   "source": [
    "You might have observed that the sentences are not of the same lengths, so we need to pad them with zeros (0's) so that the resulting array will have equal length.\n",
    "\n",
    "We'd use `text` module from `keras.preprocessing` library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d59aad7",
   "metadata": {},
   "source": [
    "We'd use a max character of 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8909766f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pad_sequences' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\PHUCKM~1\\AppData\\Local\\Temp/ipykernel_30976/4138631341.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmaxlen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m300\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mX_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist_tokenized_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmaxlen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mX_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist_tokenized_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmaxlen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pad_sequences' is not defined"
     ]
    }
   ],
   "source": [
    "maxlen = 300\n",
    "X_train = pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "X_test = pad_sequences(list_tokenized_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34cc8bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731bc39d",
   "metadata": {},
   "source": [
    "## Use pre-trained Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fcae41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load saved genism model from disk\n",
    "model_sg = Word2Vec.load(_embedding_model_filepath)\n",
    "\n",
    "# load saved numpy vectors from disk\n",
    "vectors = np.load(_pretrained_vectors_filepath)\n",
    "\n",
    "# load a saved trainable numpy vectors from disk\n",
    "vectors_neg = np.load(_neg_vectors_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda3f4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get vocabularies from saved model\n",
    "vocabs = list()\n",
    "\n",
    "for word, vocab_obj in model_sg.wv.vocab.items():\n",
    "    vocabs.append(word)\n",
    "\n",
    "# delete unused variables to free memory\n",
    "del((word, vocab_obj))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8564ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check length of vectors\n",
    "len(vectors), len(vectors_neg), len(vocabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5a54e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create word embedding dictionary\n",
    "embeddings = dict()\n",
    "\n",
    "# zip words and their corresponding vectors\n",
    "for i, (word,vector) in enumerate(zip(vocabs, vectors)):\n",
    "    embeddings[word] = vector\n",
    "    \n",
    "# delete unused variables to free memory\n",
    "del((i, word, vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882350c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tokens = vocab_size\n",
    "embedding_dim = 300\n",
    "hits = 0\n",
    "misses = 0\n",
    "\n",
    "\n",
    "# create a weight matrix for words in training docs\n",
    "embedding_matrix = np.zeros((vocab_size, 300))\n",
    "\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings.get(word)\n",
    "    \n",
    "    if embedding_vector is not None:\n",
    "        # Words not found in embedding index will be all-zeros.\n",
    "        # This includes the representation for \"padding\" and \"OOV\"\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        hits += 1\n",
    "    else:\n",
    "        misses += 1\n",
    "        \n",
    "print(\"Converted %d words (%d misses)\" % (hits, misses))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7e3bb3",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8cb21cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126fcf84",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Input(shape=(maxlen, )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27415b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 300\n",
    "model.add(Embedding(vocab_size, embed_size, weights=[embedding_matrix], trainable=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7b256b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(LSTM(60, return_sequences=True,name='lstm_layer'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4db9d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(GlobalMaxPool1D())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb5256f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dropout(0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694a3b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(50, activation=\"relu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ba4428",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dropout(0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2017706",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(3, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503665e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1621de8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00bf9bfe",
   "metadata": {},
   "source": [
    "### Create plot for model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000d9476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot model\n",
    "dot_img_file = 'model_1.png'\n",
    "tf.keras.utils.plot_model(model, to_file=dot_img_file,\n",
    "                          show_shapes=True,\n",
    "                          show_layer_names=True,\n",
    "                          rankdir=\"TB\",\n",
    "                          expand_nested=True,\n",
    "                          dpi=96\n",
    "                         )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8c8470",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bee95e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving training history\n",
    "csv_logger = CSVLogger('training.log', separator=',', append=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0170c26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "epochs = 10\n",
    "history = model.fit(X_train, Y_train, batch_size=batch_size, epochs=epochs, \n",
    "                    validation_data=(X_test, Y_test), callbacks=[csv_logger])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df02ca4a",
   "metadata": {},
   "source": [
    "### Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f3014f",
   "metadata": {},
   "outputs": [],
   "source": [
    "score, accuracy = model.evaluate(X_test, Y_test, verbose = 1, batch_size = 32)\n",
    "print(\"score: %.4f\" % (score))\n",
    "print(\"acc: %.4f\" % (accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ccf56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test, batch_size=32, verbose=1)\n",
    "\n",
    "print(\"sample of probabilistic predictions: \\n {}\".format(y_pred[:2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa13110d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# map classes to results\n",
    "predictions = np.argmax(y_pred, axis=1)\n",
    "print(\"sample of y_pred: \\n {}\".format(predictions[:2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f7393c",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = precision_score(Y_test, predictions, average='micro')\n",
    "recall = recall_score(Y_test, predictions, average='micro')\n",
    "f1 = f1_score(Y_test, predictions, average='micro')\n",
    "accuracy = accuracy_score(Y_test, predictions, normalize=True)\n",
    "print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}, Accuracy: {:.4f}\".format(precision, recall, f1, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389a6cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training history\n",
    "history = pd.read_csv('training.log', sep=',', engine='python')\n",
    "\n",
    "history.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca015e1",
   "metadata": {},
   "source": [
    "## Plot Model Accuracy & Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577fd3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot history for accuracy\n",
    "plt.plot(history[ 'accuracy' ])\n",
    "plt.plot(history[ 'val_accuracy' ])\n",
    "plt.title( 'model accuracy' )\n",
    "plt.ylabel( 'accuracy' )\n",
    "plt.xlabel( 'epoch' )\n",
    "plt.legend([ 'train' , 'test' ], loc= 'lower right' )\n",
    "plt.show()\n",
    "\n",
    "# plot history for accuracy\n",
    "plt.plot(history[ 'loss' ])\n",
    "plt.plot(history[ 'val_loss' ])\n",
    "plt.title( 'model loss' )\n",
    "plt.ylabel( 'loss' )\n",
    "plt.xlabel( 'epoch' )\n",
    "plt.legend([ 'train' , 'test' ], loc= 'upper left' )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f0db0d",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22daff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnf_matrix = confusion_matrix(Y_test, predictions)\n",
    "\n",
    "FP = cnf_matrix.sum(axis=0) - np.diag(cnf_matrix)  \n",
    "FN = cnf_matrix.sum(axis=1) - np.diag(cnf_matrix)\n",
    "TP = np.diag(cnf_matrix)\n",
    "TN = cnf_matrix.sum() - (FP + FN + TP)\n",
    "\n",
    "FP = FP.astype(float)\n",
    "FN = FN.astype(float)\n",
    "TP = TP.astype(float)\n",
    "TN = TN.astype(float)\n",
    "\n",
    "# Sensitivity, hit rate, recall, or true positive rate\n",
    "TPR = TP/(TP+FN)\n",
    "# Specificity or true negative rate\n",
    "TNR = TN/(TN+FP) \n",
    "# Precision or positive predictive value\n",
    "PPV = TP/(TP+FP)\n",
    "# Negative predictive value\n",
    "NPV = TN/(TN+FN)\n",
    "# Fall out or false positive rate\n",
    "FPR = FP/(FP+TN)\n",
    "# False negative rate\n",
    "FNR = FN/(TP+FN)\n",
    "# False discovery rate\n",
    "FDR = FP/(TP+FP)\n",
    "# Overall accuracy\n",
    "ACC = (TP+TN)/(TP+FP+FN+TN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755ef937",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(TP, FP, TN, FN)\n",
    "print(TPR, TNR, PPV, NPV, FPR, FNR, FDR, ACC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24709c9",
   "metadata": {},
   "source": [
    "## Plot ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b7c4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(FPR, TPR, linestyle='--', label='No Skill')\n",
    "plt.plot(FPR, TPR, marker='.', label='LSTM')\n",
    "\n",
    "# axis labels\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "# show the legend\n",
    "plt.legend()\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7051c5d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
