{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3fc097b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd4cad8",
   "metadata": {},
   "source": [
    "## Import relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eadcb4a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim==3.8.1 in c:\\users\\phuckmanity\\ds\\sentiment_analysis_hausa\\venv\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\phuckmanity\\ds\\sentiment_analysis_hausa\\venv\\lib\\site-packages (from gensim==3.8.1) (5.2.1)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\phuckmanity\\ds\\sentiment_analysis_hausa\\venv\\lib\\site-packages (from gensim==3.8.1) (1.4.1)\n",
      "Requirement already satisfied: six>=1.5.0 in c:\\users\\phuckmanity\\ds\\sentiment_analysis_hausa\\venv\\lib\\site-packages (from gensim==3.8.1) (1.15.0)\n",
      "Requirement already satisfied: numpy>=1.11.3 in c:\\users\\phuckmanity\\ds\\sentiment_analysis_hausa\\venv\\lib\\site-packages (from gensim==3.8.1) (1.19.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim==3.8.1\n",
    "\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7680b9a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created `%t` as an alias for `%timeit`.\n",
      "Created `%%t` as an alias for `%%timeit`.\n"
     ]
    }
   ],
   "source": [
    "# core system imports\n",
    "import os\n",
    "\n",
    "import sys, os, re, csv, codecs, numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import string\n",
    "import random\n",
    "import joblib\n",
    "from unidecode import unidecode\n",
    "import itertools\n",
    "from timeit import timeit\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import (\n",
    "    Dense, \n",
    "    Input, \n",
    "    LSTM, \n",
    "    Embedding, \n",
    "    Dropout, \n",
    "    Activation, \n",
    "    Bidirectional, \n",
    "    GlobalMaxPool1D\n",
    ")\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras import (\n",
    "    initializers, \n",
    "    regularizers, \n",
    "    constraints, \n",
    "    optimizers, \n",
    "    layers\n",
    ")\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    accuracy_score,\n",
    "    precision_recall_fscore_support\n",
    ")\n",
    "\n",
    "# Matplotlib config\n",
    "%matplotlib inline\n",
    "%alias_magic t timeit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "aad9ccbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "# check for available GPU\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4fe767c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Location of the pipeline metadata store\n",
    "_pipeline_root = '../pipeline/'\n",
    "\n",
    "# Directory of the raw data files\n",
    "_data_root = '../input/'\n",
    "\n",
    "# Directory of the pretrained word embeddings\n",
    "_embedding_root = '../input/embeddings'\n",
    "\n",
    "_data_filepath = os.path.join(_data_root, \"data.csv\")\n",
    "_stopwords_filepath = os.path.join(_data_root, \"stopwords.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c70a3c84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['model_sg', 'model_sg.trainables.syn1neg.npy', 'model_sg.wv.vectors.npy']\n"
     ]
    }
   ],
   "source": [
    "# List input datasets in directory\n",
    "os.listdir(_data_root)\n",
    "\n",
    "# Word embeddings\n",
    "print(os.listdir(_embedding_root))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e4fecb",
   "metadata": {},
   "source": [
    "## Load training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fe9bed6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from CSV file\n",
    "data = pd.read_csv(_data_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "03b20ce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 99 entries, 0 to 98\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   text       99 non-null     object \n",
      " 1   author_id  99 non-null     float64\n",
      " 2   Label      99 non-null     object \n",
      "dtypes: float64(1), object(2)\n",
      "memory usage: 2.4+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "01d03f3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>author_id</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ishmbuhri t nemi bbbn sufeton yn snd d y mgnc...</td>\n",
       "      <td>7.970000e+17</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>duk dn bokon d bid ilimin ddini nnob ne cikin ...</td>\n",
       "      <td>2.290470e+09</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>duk mutumin d yyi tunnin bw mutne ilimi d koy ...</td>\n",
       "      <td>1.071387e+09</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>duk wnd y sbw dokr kucew kmuw dg cutr coron zi...</td>\n",
       "      <td>1.260000e+18</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>duk wnd y sn y fito dg ynkin d ke nnobr coron ...</td>\n",
       "      <td>1.039268e+09</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text     author_id     Label\n",
       "0   ishmbuhri t nemi bbbn sufeton yn snd d y mgnc...  7.970000e+17   Neutral\n",
       "1  duk dn bokon d bid ilimin ddini nnob ne cikin ...  2.290470e+09   Neutral\n",
       "2  duk mutumin d yyi tunnin bw mutne ilimi d koy ...  1.071387e+09   Neutral\n",
       "3  duk wnd y sbw dokr kucew kmuw dg cutr coron zi...  1.260000e+18  Positive\n",
       "4  duk wnd y sn y fito dg ynkin d ke nnobr coron ...  1.039268e+09  Positive"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9c6be05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read stop words\n",
    "stopwords_list = list()\n",
    "\n",
    "with open(_stopwords_filepath) as file:\n",
    "    stopwords_list = [line.strip() for line in file.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6400176a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ta da ya sai ba yi na kuma ma ji cikin in ni wata wani ce tana don za sun amma ga ina ne mai suka wannan a ko lokacin su take shi yake yana ka ban ita tafi\n"
     ]
    }
   ],
   "source": [
    "listToStr = ' '.join([str(word) for word in stopwords_list])\n",
    "print(listToStr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8aa6539c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read saved data from disk\n",
    "def load_pickle(filename):\n",
    "    data = joblib.load(filename)\n",
    "    return(data)\n",
    "    \n",
    "# Save data to disk for future use\n",
    "def save_pickle(data, filename):\n",
    "    joblib.dump(data, filename)\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ef00641c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing stopwords\n",
    "def _apply_lowercase(text):\n",
    "    text = [item for item in text if item not in stopwords_list]\n",
    "    text = ''.join(text)\n",
    "    return text\n",
    "\n",
    "# removing stopwords\n",
    "def _stopwords_removal(text):\n",
    "    text = [item for item in text if item not in stopwords_list]\n",
    "    text = ''.join(text)\n",
    "    return text\n",
    "\n",
    "# remove punctuations\n",
    "def _punctuation_removal(text):\n",
    "    all_list = [char for char in text if char not in string.punctuation]\n",
    "    clean_str = ''.join(all_list)\n",
    "    return clean_str\n",
    "\n",
    "# Shuffle dataset\n",
    "def _shuffle_dataset(dataset):\n",
    "    dataset = shuffle(data)\n",
    "    dataset = data.reset_index(drop=True)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c5f914d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>author_id</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ishmbuhri t nemi bbbn sufeton yn snd d y mgnc...</td>\n",
       "      <td>7.970000e+17</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>duk dn bokon d bid ilimin ddini nnob ne cikin ...</td>\n",
       "      <td>2.290470e+09</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>duk mutumin d yyi tunnin bw mutne ilimi d koy ...</td>\n",
       "      <td>1.071387e+09</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>duk wnd y sbw dokr kucew kmuw dg cutr coron zi...</td>\n",
       "      <td>1.260000e+18</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>duk wnd y sn y fito dg ynkin d ke nnobr coron ...</td>\n",
       "      <td>1.039268e+09</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text     author_id     Label\n",
       "0   ishmbuhri t nemi bbbn sufeton yn snd d y mgnc...  7.970000e+17   Neutral\n",
       "1  duk dn bokon d bid ilimin ddini nnob ne cikin ...  2.290470e+09   Neutral\n",
       "2  duk mutumin d yyi tunnin bw mutne ilimi d koy ...  1.071387e+09   Neutral\n",
       "3  duk wnd y sbw dokr kucew kmuw dg cutr coron zi...  1.260000e+18  Positive\n",
       "4  duk wnd y sn y fito dg ynkin d ke nnobr coron ...  1.039268e+09  Positive"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Change string to lower case\n",
    "data['text'] = data['text'].apply(str.lower)\n",
    "\n",
    "# remove punctuations or special characters\n",
    "data['text'] = data['text'].apply(_punctuation_removal)\n",
    "\n",
    "# remove stopwords\n",
    "data['text'] = data['text'].apply(_stopwords_removal)\n",
    "\n",
    "# Shuffle the dataset to prevent bias:\n",
    "data = _shuffle_dataset(data)\n",
    "\n",
    "# Print head of the data\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb16d65",
   "metadata": {},
   "source": [
    "### Split Dataset\n",
    "\n",
    "+ Tranining: 70% of the dataset\n",
    "+ Testing: 30% of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "078ec857",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((69,), (30,))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_classes = [\"Label\"]\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(data[\"text\"], data[list_classes], test_size=0.3, random_state = 1)\n",
    "\n",
    "Y_train = Y_train.values\n",
    "Y_test = Y_test.values\n",
    "\n",
    "# Show dimension of the comments\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb88c273",
   "metadata": {},
   "source": [
    "### Tokenize Sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ba027e",
   "metadata": {},
   "source": [
    "To be able to train our model with a text data, we'd have to convert it into number form, for this we're going to use the Tokenizer module from Keras.preprocessing library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5fce9302",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_sentences_train = X_train.values\n",
    "list_sentences_test = X_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0188ca71",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(lower=True)\n",
    "\n",
    "tokenizer.fit_on_texts(list(list_sentences_train))\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b1cfc3c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "754"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# size of training vocabulary (number of unique words)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "32be4f77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[181, 61, 1, 14, 1, 9, 1, 15, 3, 4, 182, 90]\n"
     ]
    }
   ],
   "source": [
    "# print out a random sequence of text from the tokenized training data\n",
    "print(random.choice(list_tokenized_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5f29ca5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69 train sequences\n",
      "30 test sequences\n",
      "Average train sequence length: 20\n",
      "Average test sequence length: 10\n"
     ]
    }
   ],
   "source": [
    "print(len(list_tokenized_train), 'train sequences')\n",
    "print(len(list_tokenized_test), 'test sequences')\n",
    "\n",
    "print('Average train sequence length: {}'.format(np.mean(list(map(len, list_tokenized_train)), dtype=int)))\n",
    "print('Average test sequence length: {}'.format(np.mean(list(map(len, list_tokenized_test)), dtype=int)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d97b93",
   "metadata": {},
   "source": [
    "## Pad tokenized Sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb53a50",
   "metadata": {},
   "source": [
    "You might have observed that the sentences are not of the same lengths, so we need to pad them with zeros (0's) so that the resulting array will have equal length.\n",
    "\n",
    "We'd use `text` module from `keras.preprocessing` library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d59aad7",
   "metadata": {},
   "source": [
    "We'd use a max character of 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8909766f",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 300\n",
    "X_train = pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "X_test = pad_sequences(list_tokenized_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "34cc8bcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0 175  89 176 177 178  60 179 180   3   4]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731bc39d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "66c555e3",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
