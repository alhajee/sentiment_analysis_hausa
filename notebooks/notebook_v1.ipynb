{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3fc097b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd4cad8",
   "metadata": {},
   "source": [
    "## Import relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eadcb4a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim==3.8.1 in c:\\users\\phuckmanity\\ds\\sentiment_analysis_hausa\\venv\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\phuckmanity\\ds\\sentiment_analysis_hausa\\venv\\lib\\site-packages (from gensim==3.8.1) (5.2.1)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\phuckmanity\\ds\\sentiment_analysis_hausa\\venv\\lib\\site-packages (from gensim==3.8.1) (1.4.1)\n",
      "Requirement already satisfied: six>=1.5.0 in c:\\users\\phuckmanity\\ds\\sentiment_analysis_hausa\\venv\\lib\\site-packages (from gensim==3.8.1) (1.15.0)\n",
      "Requirement already satisfied: numpy>=1.11.3 in c:\\users\\phuckmanity\\ds\\sentiment_analysis_hausa\\venv\\lib\\site-packages (from gensim==3.8.1) (1.19.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim==3.8.1\n",
    "\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7680b9a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created `%t` as an alias for `%timeit`.\n",
      "Created `%%t` as an alias for `%%timeit`.\n"
     ]
    }
   ],
   "source": [
    "# core system imports\n",
    "import os\n",
    "\n",
    "import sys, os, re, csv, codecs, numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import string\n",
    "import random\n",
    "import joblib\n",
    "from unidecode import unidecode\n",
    "import itertools\n",
    "from timeit import timeit\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import (\n",
    "    Dense, \n",
    "    Input, \n",
    "    LSTM, \n",
    "    Embedding, \n",
    "    Dropout, \n",
    "    Activation, \n",
    "    Bidirectional, \n",
    "    GlobalMaxPool1D\n",
    ")\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras import (\n",
    "    initializers, \n",
    "    regularizers, \n",
    "    constraints, \n",
    "    optimizers, \n",
    "    layers\n",
    ")\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    accuracy_score,\n",
    "    precision_recall_fscore_support\n",
    ")\n",
    "\n",
    "# Matplotlib config\n",
    "%matplotlib inline\n",
    "%alias_magic t timeit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "aad9ccbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "# check for available GPU\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4fe767c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Location of the pipeline metadata store\n",
    "_pipeline_root = '../pipeline/'\n",
    "\n",
    "# Directory of the raw data files\n",
    "_data_root = '../input/'\n",
    "\n",
    "# Directory of the pretrained word embeddings\n",
    "_embedding_root = '../input/embeddings'\n",
    "\n",
    "_data_filepath = os.path.join(_data_root, \"data.csv\")\n",
    "\n",
    "_stopwords_filepath = os.path.join(_data_root, \"stopwords.txt\")\n",
    "\n",
    "_embedding_model_filepath = os.path.join(_embedding_root, \"model_sg\")\n",
    "_pretrained_vectors_filepath = os.path.join(_embedding_root, \"model_sg.wv.vectors.npy\")\n",
    "_neg_vectors_filepath = os.path.join(_embedding_root, \"model_sg.trainables.syn1neg.npy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c70a3c84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['model_sg', 'model_sg.trainables.syn1neg.npy', 'model_sg.wv.vectors.npy']\n"
     ]
    }
   ],
   "source": [
    "# List input datasets in directory\n",
    "os.listdir(_data_root)\n",
    "\n",
    "# Word embeddings\n",
    "print(os.listdir(_embedding_root))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e4fecb",
   "metadata": {},
   "source": [
    "## Load training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fe9bed6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from CSV file\n",
    "data = pd.read_csv(_data_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "03b20ce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 99 entries, 0 to 98\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   text       99 non-null     object \n",
      " 1   author_id  99 non-null     float64\n",
      " 2   Label      99 non-null     object \n",
      "dtypes: float64(1), object(2)\n",
      "memory usage: 2.4+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "01d03f3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>author_id</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ishmbuhri t nemi bbbn sufeton yn snd d y mgnc...</td>\n",
       "      <td>7.970000e+17</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>duk dn bokon d bid ilimin ddini nnob ne cikin ...</td>\n",
       "      <td>2.290470e+09</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>duk mutumin d yyi tunnin bw mutne ilimi d koy ...</td>\n",
       "      <td>1.071387e+09</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>duk wnd y sbw dokr kucew kmuw dg cutr coron zi...</td>\n",
       "      <td>1.260000e+18</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>duk wnd y sn y fito dg ynkin d ke nnobr coron ...</td>\n",
       "      <td>1.039268e+09</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text     author_id     Label\n",
       "0   ishmbuhri t nemi bbbn sufeton yn snd d y mgnc...  7.970000e+17   Neutral\n",
       "1  duk dn bokon d bid ilimin ddini nnob ne cikin ...  2.290470e+09   Neutral\n",
       "2  duk mutumin d yyi tunnin bw mutne ilimi d koy ...  1.071387e+09   Neutral\n",
       "3  duk wnd y sbw dokr kucew kmuw dg cutr coron zi...  1.260000e+18  Positive\n",
       "4  duk wnd y sn y fito dg ynkin d ke nnobr coron ...  1.039268e+09  Positive"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9c6be05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read stop words\n",
    "stopwords_list = list()\n",
    "\n",
    "with open(_stopwords_filepath) as file:\n",
    "    stopwords_list = [line.strip() for line in file.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6400176a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ta da ya sai ba yi na kuma ma ji cikin in ni wata wani ce tana don za sun amma ga ina ne mai suka wannan a ko lokacin su take shi yake yana ka ban ita tafi\n"
     ]
    }
   ],
   "source": [
    "listToStr = ' '.join([str(word) for word in stopwords_list])\n",
    "print(listToStr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8aa6539c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read saved data from disk\n",
    "def load_pickle(filename):\n",
    "    data = joblib.load(filename)\n",
    "    return(data)\n",
    "    \n",
    "# Save data to disk for future use\n",
    "def save_pickle(data, filename):\n",
    "    joblib.dump(data, filename)\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ef00641c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing stopwords\n",
    "def _apply_lowercase(text):\n",
    "    text = [item for item in text if item not in stopwords_list]\n",
    "    text = ''.join(text)\n",
    "    return text\n",
    "\n",
    "# removing stopwords\n",
    "def _stopwords_removal(text):\n",
    "    text = [item for item in text if item not in stopwords_list]\n",
    "    text = ''.join(text)\n",
    "    return text\n",
    "\n",
    "# remove punctuations\n",
    "def _punctuation_removal(text):\n",
    "    all_list = [char for char in text if char not in string.punctuation]\n",
    "    clean_str = ''.join(all_list)\n",
    "    return clean_str\n",
    "\n",
    "# Shuffle dataset\n",
    "def _shuffle_dataset(dataset):\n",
    "    dataset = shuffle(data)\n",
    "    dataset = data.reset_index(drop=True)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c5f914d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>author_id</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ishmbuhri t nemi bbbn sufeton yn snd d y mgnc...</td>\n",
       "      <td>7.970000e+17</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>duk dn bokon d bid ilimin ddini nnob ne cikin ...</td>\n",
       "      <td>2.290470e+09</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>duk mutumin d yyi tunnin bw mutne ilimi d koy ...</td>\n",
       "      <td>1.071387e+09</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>duk wnd y sbw dokr kucew kmuw dg cutr coron zi...</td>\n",
       "      <td>1.260000e+18</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>duk wnd y sn y fito dg ynkin d ke nnobr coron ...</td>\n",
       "      <td>1.039268e+09</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text     author_id     Label\n",
       "0   ishmbuhri t nemi bbbn sufeton yn snd d y mgnc...  7.970000e+17   Neutral\n",
       "1  duk dn bokon d bid ilimin ddini nnob ne cikin ...  2.290470e+09   Neutral\n",
       "2  duk mutumin d yyi tunnin bw mutne ilimi d koy ...  1.071387e+09   Neutral\n",
       "3  duk wnd y sbw dokr kucew kmuw dg cutr coron zi...  1.260000e+18  Positive\n",
       "4  duk wnd y sn y fito dg ynkin d ke nnobr coron ...  1.039268e+09  Positive"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Change string to lower case\n",
    "data['text'] = data['text'].apply(str.lower)\n",
    "\n",
    "# remove punctuations or special characters\n",
    "data['text'] = data['text'].apply(_punctuation_removal)\n",
    "\n",
    "# remove stopwords\n",
    "data['text'] = data['text'].apply(_stopwords_removal)\n",
    "\n",
    "# Shuffle the dataset to prevent bias:\n",
    "data = _shuffle_dataset(data)\n",
    "\n",
    "# Print head of the data\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb16d65",
   "metadata": {},
   "source": [
    "### Split Dataset\n",
    "\n",
    "+ Tranining: 70% of the dataset\n",
    "+ Testing: 30% of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "078ec857",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((69,), (30,))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_classes = [\"Label\"]\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(data[\"text\"], data[list_classes], test_size=0.3, random_state = 1)\n",
    "\n",
    "Y_train = Y_train.values\n",
    "Y_test = Y_test.values\n",
    "\n",
    "# Show dimension of the comments\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb88c273",
   "metadata": {},
   "source": [
    "### Tokenize Sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ba027e",
   "metadata": {},
   "source": [
    "To be able to train our model with a text data, we'd have to convert it into number form, for this we're going to use the Tokenizer module from Keras.preprocessing library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5fce9302",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_sentences_train = X_train.values\n",
    "list_sentences_test = X_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0188ca71",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(lower=True)\n",
    "\n",
    "tokenizer.fit_on_texts(list(list_sentences_train))\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b1cfc3c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "754"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# size of training vocabulary (number of unique words)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "32be4f77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[181, 61, 1, 14, 1, 9, 1, 15, 3, 4, 182, 90]\n"
     ]
    }
   ],
   "source": [
    "# print out a random sequence of text from the tokenized training data\n",
    "print(random.choice(list_tokenized_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5f29ca5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69 train sequences\n",
      "30 test sequences\n",
      "Average train sequence length: 20\n",
      "Average test sequence length: 10\n"
     ]
    }
   ],
   "source": [
    "print(len(list_tokenized_train), 'train sequences')\n",
    "print(len(list_tokenized_test), 'test sequences')\n",
    "\n",
    "print('Average train sequence length: {}'.format(np.mean(list(map(len, list_tokenized_train)), dtype=int)))\n",
    "print('Average test sequence length: {}'.format(np.mean(list(map(len, list_tokenized_test)), dtype=int)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d97b93",
   "metadata": {},
   "source": [
    "## Pad tokenized Sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb53a50",
   "metadata": {},
   "source": [
    "You might have observed that the sentences are not of the same lengths, so we need to pad them with zeros (0's) so that the resulting array will have equal length.\n",
    "\n",
    "We'd use `text` module from `keras.preprocessing` library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d59aad7",
   "metadata": {},
   "source": [
    "We'd use a max character of 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8909766f",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 300\n",
    "X_train = pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "X_test = pad_sequences(list_tokenized_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "34cc8bcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0 175  89 176 177 178  60 179 180   3   4]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731bc39d",
   "metadata": {},
   "source": [
    "## Use pre-trained Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6fcae41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load saved genism model from disk\n",
    "model_sg = Word2Vec.load(_embedding_model_filepath)\n",
    "\n",
    "# load saved numpy vectors from disk\n",
    "vectors = np.load(_pretrained_vectors_filepath)\n",
    "\n",
    "# load a saved trainable numpy vectors from disk\n",
    "vectors_neg = np.load(_neg_vectors_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "cda3f4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get vocabularies from saved model\n",
    "vocabs = list()\n",
    "\n",
    "for word, vocab_obj in model_sg.wv.vocab.items():\n",
    "    vocabs.append(word)\n",
    "\n",
    "# delete unused variables to free memory\n",
    "del((word, vocab_obj))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a8564ddf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(90451, 90451, 90451)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check length of vectors\n",
    "len(vectors), len(vectors_neg), len(vocabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7a5a54e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create word embedding dictionary\n",
    "embeddings = dict()\n",
    "\n",
    "# zip words and their corresponding vectors\n",
    "for i, (word,vector) in enumerate(zip(vocabs, vectors)):\n",
    "    embeddings[word] = vector\n",
    "    \n",
    "# delete unused variables to free memory\n",
    "del((i, word, vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "882350c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 539 words (214 misses)\n"
     ]
    }
   ],
   "source": [
    "num_tokens = vocab_size\n",
    "embedding_dim = 300\n",
    "hits = 0\n",
    "misses = 0\n",
    "\n",
    "\n",
    "# create a weight matrix for words in training docs\n",
    "embedding_matrix = np.zeros((vocab_size, 300))\n",
    "\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings.get(word)\n",
    "    \n",
    "    if embedding_vector is not None:\n",
    "        # Words not found in embedding index will be all-zeros.\n",
    "        # This includes the representation for \"padding\" and \"OOV\"\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        hits += 1\n",
    "    else:\n",
    "        misses += 1\n",
    "        \n",
    "print(\"Converted %d words (%d misses)\" % (hits, misses))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7e3bb3",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f8cb21cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "126fcf84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n"
     ]
    }
   ],
   "source": [
    "model.add(Input(shape=(maxlen, )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "27415b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 300\n",
    "model.add(Embedding(vocab_size, embed_size, weights=[embedding_matrix], trainable=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "8f7b256b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(LSTM(60, return_sequences=True,name='lstm_layer'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f4db9d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(GlobalMaxPool1D())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "deb5256f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dropout(0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "694a3b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(50, activation=\"relu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "41ba4428",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dropout(0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d2017706",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(1, activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "503665e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "1621de8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 300, 300)          226200    \n",
      "_________________________________________________________________\n",
      "lstm_layer (LSTM)            (None, 300, 60)           86640     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 60)                0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 60)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 50)                3050      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 315,941\n",
      "Trainable params: 89,741\n",
      "Non-trainable params: 226,200\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00bf9bfe",
   "metadata": {},
   "source": [
    "### Create plot for model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "000d9476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) ', 'for plot_model/model_to_dot to work.')\n"
     ]
    }
   ],
   "source": [
    "# Plot model\n",
    "dot_img_file = 'model_1.png'\n",
    "tf.keras.utils.plot_model(model, to_file=dot_img_file,\n",
    "                          show_shapes=True,\n",
    "                          show_layer_names=True,\n",
    "                          rankdir=\"TB\",\n",
    "                          expand_nested=True,\n",
    "                          dpi=96\n",
    "                         )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8c8470",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "0170c26c",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnimplementedError",
     "evalue": " Cast string to float is not supported\n\t [[node binary_crossentropy/Cast (defined at C:\\Users\\PHUCKM~1\\AppData\\Local\\Temp/ipykernel_20752/479073775.py:3) ]] [Op:__inference_train_function_4405]\n\nFunction call stack:\ntrain_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnimplementedError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\PHUCKM~1\\AppData\\Local\\Temp/ipykernel_20752/479073775.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m16\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\PhuckManity\\DS\\sentiment_analysis_hausa\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1181\u001b[0m                 _r=1):\n\u001b[0;32m   1182\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1183\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1184\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1185\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\PhuckManity\\DS\\sentiment_analysis_hausa\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    887\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\PhuckManity\\DS\\sentiment_analysis_hausa\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    948\u001b[0m         \u001b[1;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m         \u001b[1;31m# stateless function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 950\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    951\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    952\u001b[0m       \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfiltered_flat_args\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\PhuckManity\\DS\\sentiment_analysis_hausa\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m   3023\u001b[0m     return graph_function._call_flat(\n\u001b[1;32m-> 3024\u001b[1;33m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m   3025\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3026\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\PhuckManity\\DS\\sentiment_analysis_hausa\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1959\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1960\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1961\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1963\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\PhuckManity\\DS\\sentiment_analysis_hausa\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 596\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    597\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32mc:\\Users\\PhuckManity\\DS\\sentiment_analysis_hausa\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnimplementedError\u001b[0m:  Cast string to float is not supported\n\t [[node binary_crossentropy/Cast (defined at C:\\Users\\PHUCKM~1\\AppData\\Local\\Temp/ipykernel_20752/479073775.py:3) ]] [Op:__inference_train_function_4405]\n\nFunction call stack:\ntrain_function\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "epochs = 1\n",
    "model.fit(X_train, Y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df02ca4a",
   "metadata": {},
   "source": [
    "### Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f3014f",
   "metadata": {},
   "outputs": [],
   "source": [
    "score, accuracy = model.evaluate(X_test, Y_test, verbose = 1, batch_size = 32)\n",
    "print(\"score: %.4f\" % (score))\n",
    "print(\"acc: %.4f\" % (accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ccf56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test, batch_size=32, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f03abeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# classify the input image then find the indexes of the two class\n",
    "# labels with the *largest* probability\n",
    "index = 0\n",
    "\n",
    "print(\"[INFO] classifying tweet {}\".format(index))\n",
    "\n",
    "proba = predictions[index]\n",
    "idxs = np.argsort(proba)[::-1][:2]\n",
    "\n",
    "# loop over the indexes of the high confidence class labels\n",
    "for (i, j) in enumerate(idxs):\n",
    "    # build the label and draw the label on the image\n",
    "    label = \"{}: {:.2f}%\".format(list_classes[j], proba[j] * 100)\n",
    "    \n",
    "# show the probabilities for each of the individual labels\n",
    "for (label, p) in zip(list_classes, proba):\n",
    "    print(\"{}: {:.2f}%\".format(label, p * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee537845",
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds=[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n",
    "for val in thresholds:\n",
    "    print(\"For threshold: \", val)\n",
    "    pred=predictions.copy()\n",
    "  \n",
    "    pred[pred>=val]=1\n",
    "    pred[pred<val]=0\n",
    "  \n",
    "    precision = precision_score(Y_test, pred, average='micro')\n",
    "    recall = recall_score(Y_test, pred, average='micro')\n",
    "    f1 = f1_score(Y_test, pred, average='micro')\n",
    "    accuracy = accuracy_score(Y_test, pred, normalize=True)\n",
    "\n",
    "    print(\"Micro-average quality numbers\")\n",
    "    print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}, Accuracy: {:.4f}\".format(precision, recall, f1, accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c555e3",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
